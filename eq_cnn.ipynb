{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN untuk data gempabumi \n",
    "## Klasifikasi noise dan event gempabumi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init work folder\n",
    "\n",
    "import os #library untuk mengakses atau berinteraksi dengan Operating System laptop kita\n",
    "\n",
    "global directory\n",
    "directory = os.getcwd()\n",
    "\n",
    "#print (directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baca semua dataset di folder dataset (noise dan gempa)\n",
    "#kita buat fungsi untuk membuat labels dataset dengan output files csv \"labels.csv\"\n",
    "\n",
    "import pandas as pd #library untuk penanganan data tabel\n",
    "\n",
    "def make_labels_csv():\n",
    "    \n",
    "    #jika ada file labels.csv hapus lebih dahulu \n",
    "    if os.path.exists(directory+'/labels.csv'):\n",
    "        os.remove(directory+'/labels.csv')\n",
    "\n",
    "    #headar tabel berupa nama file dan labelsnya dan simpan dalam bentuk csv\n",
    "    df = pd.DataFrame(columns=['files_name', 'labels'])\n",
    "    df.to_csv(directory+'/labels.csv', mode='w', header=True, index=False)\n",
    "\n",
    "    #lakukan looping pada setiap folder lalu simpan \n",
    "    for r,d,files in os.walk(directory+'/dataset/event/'):\n",
    "        i = 0\n",
    "        while i < len(files):\n",
    "            data_ = {\n",
    "                'files_name'    :   [files[i]],\n",
    "                'labels'        :   ['event']\n",
    "                }\n",
    "            i += 1\n",
    "\n",
    "            df = pd.DataFrame(data_)\n",
    "            df.to_csv(directory+'/labels.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    for r,d,files in os.walk(directory+'/dataset/noise/'):\n",
    "        i = 0\n",
    "        while i < len(files):\n",
    "            data_ = {\n",
    "                'files_name'    :   [files[i]],\n",
    "                'labels'        :   ['noise']\n",
    "                }\n",
    "            i += 1\n",
    "\n",
    "            df = pd.DataFrame(data_)\n",
    "            df.to_csv(directory+'/labels.csv', mode='a', header=False, index=False)\n",
    "\n",
    "#make labels\n",
    "make_labels_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cek data waveform mseed menggunakan library obspy\n",
    "## yang perlu dicek adalah data array jumlah samples nya, ini akan berpengaruh pada input model\n",
    "## Pada inputan model CNN, 1D atau pun 2D harus memiliki bentuk (Shape) yang sama antar data.\n",
    "## Sehingga untuk memudahkan inputan data pada model CNN, alternatif cara yang dilakukan ialah merubah bentuk data (Spectogram/Image atau Array )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import read\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def readwaveform(path):\n",
    "    if os.path.exists(directory+'/dataset/spectogram/'):\n",
    "        os.rmdir(directory+'/dataset/spectogram/')\n",
    "    \n",
    "    os.mkdir(directory+'/dataset/spectogram/')\n",
    "\n",
    "    #baca csv dan inisiasi header dan ubah kedalam bentuk array\n",
    "    read_csv = pd.read_csv(directory+'/'+path)\n",
    "    init_stations = read_csv[['files_name','labels']]\n",
    "    arr_init_stations = init_stations.to_numpy()\n",
    "    \n",
    "    for i in arr_init_stations:\n",
    "        st = read(directory+'/dataset/'+i[1]+'/'+i[0]).resample(100)\n",
    "        # print(st.__str__(extended=True))\n",
    "        data = st[0].data.astype('float32')\n",
    "        sr = int(st[0].stats.sampling_rate)\n",
    "        \n",
    "        # print(data)\n",
    "        # rubah bentuk domain waktu ke domain frekuensi menggunkan spectogram\n",
    "        fig, ax = plt.subplots(figsize=(3,2))\n",
    "        ax.specgram(data,Fs=100,NFFT=256,cmap='gray',vmin=-10,vmax=25);\n",
    "        ax.set_xlim([0,60])\n",
    "        ax.axis('off')\n",
    "        plt.gca().set_axis_off()\n",
    "        plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)\n",
    "        plt.margins(0,0)\n",
    "        plt.savefig(directory+'/dataset/spectogram/'+i[0]+'.png',bbox_inches='tight',transparent = True,pad_inches=0,dpi=50) #simpan kedalam folderxs\n",
    "        plt.close()\n",
    "            \n",
    "        \n",
    "readwaveform('labels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class HParams(object):\n",
    "    \"\"\" Hparams was removed from tf 2.0alpha so this is a placeholder\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "@tf.function\n",
    "def _normalize_tensorflow(S, hparams):\n",
    "        return tf.clip_by_value((S - hparams.min_level_db) / -hparams.min_level_db, 0, 1)\n",
    "\n",
    "def _tf_log10(x):\n",
    "        numerator = tf.math.log(x)\n",
    "        denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
    "        return numerator / denominator\n",
    "\n",
    "def _amp_to_db_tensorflow(x):\n",
    "        return 20 * _tf_log10(tf.clip_by_value(tf.abs(x), 1e-5, 1e100))\n",
    "\n",
    "def _stft_tensorflow(signals, hparams):\n",
    "        return tf.signal.stft(\n",
    "            signals,\n",
    "            hparams.win_length,\n",
    "            hparams.hop_length,\n",
    "            hparams.n_fft,\n",
    "            pad_end=True,\n",
    "            window_fn=tf.signal.hann_window,\n",
    "        )\n",
    "\n",
    "def spectrogram_tensorflow(y, hparams):\n",
    "        D = _stft_tensorflow(y, hparams)\n",
    "        S = _amp_to_db_tensorflow(tf.abs(D)) - hparams.ref_level_db\n",
    "        return _normalize_tensorflow(S, hparams)\n",
    "\n",
    "\n",
    "def spectogramtwo(path):\n",
    "    if os.path.exists(directory+'/dataset/spectogram_2/'):\n",
    "        os.rmdir(directory+'/dataset/spectogram_2/')\n",
    "    \n",
    "    os.mkdir(directory+'/dataset/spectogram_2/')\n",
    "    \n",
    "    read_csv = pd.read_csv(directory+'/'+path)\n",
    "    init_stations = read_csv[['files_name','labels']]\n",
    "    arr_init_stations = init_stations.to_numpy()\n",
    "    \n",
    "    for i in arr_init_stations:\n",
    "        st = read(directory+'/dataset/'+i[1]+'/'+i[0]).resample(100)\n",
    "            # print(st.__str__(extended=True))\n",
    "        data = st[0].data.astype('float32')\n",
    "        sr = int(st[0].stats.sampling_rate)\n",
    "\n",
    "        hparams = HParams(  \n",
    "                # spectrogramming\n",
    "                win_length = 256,\n",
    "                n_fft = 256,\n",
    "                hop_length= 128,\n",
    "                ref_level_db = 250000,\n",
    "                min_level_db = -250000,\n",
    "                # mel scaling\n",
    "                num_mel_bins = 128,\n",
    "                mel_lower_edge_hertz = 0,\n",
    "                mel_upper_edge_hertz = (sr/2),\n",
    "                # inversion\n",
    "                power = 1, # for spectral inversion\n",
    "                griffin_lim_iters = 50,\n",
    "                pad=True,\n",
    "                #\n",
    "            )\n",
    "        \n",
    "        spectrogram = spectrogram_tensorflow(data.astype('float32'), hparams)\n",
    "        fig, ax = plt.subplots(ncols=1, figsize=(15,4))\n",
    "        cax = ax.matshow(spectrogram.numpy().T, aspect='auto', origin='lower')\n",
    "        # fig.colorbar(cax)\n",
    "        ax.axis('off')\n",
    "        plt.savefig(directory+'/dataset/spectogram_2/'+i[0]+'.png', bbox_inches='tight', dpi=50)\n",
    "        plt.close\n",
    "        \n",
    "        \n",
    "spectogramtwo('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('labels.csv')\n",
    "train_df= train_df[['files_name','labels']]\n",
    "train_df['spectogram'] = train_df.files_name.map(lambda id: f'dataset/spectogram_2/{id}.png')\n",
    "train_df['labels_num'] = train_df.labels.map(lambda x : 1 if x == 'event' else 0) \n",
    "# print(train_df.head())\n",
    "\n",
    "print(train_df['spectogram'])\n",
    "print(train_df['labels_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baca image untuk mendapatkan informasi bentuk\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_spectogram(path):\n",
    "    images = []\n",
    "    for file_path in path:\n",
    "        img = imread(file_path)\n",
    "        width = int(img.shape[1])\n",
    "        height = int(img.shape[0])\n",
    "        dim = (width, height)\n",
    "        img_resize = cv2.resize(img, dim)\n",
    "        images.append(img_resize)\n",
    "    images = np.asarray(images, dtype=np.float32)\n",
    "    images = images / 255.0\n",
    "    images.shape\n",
    "    # images = images.reshape(images.shape[1], images.shape[2],1)\n",
    "    \n",
    "    input_shape = images.shape \n",
    "    # print(f'image is {channels} \\n {images.shape}')\n",
    "\n",
    "    return images, input_shape\n",
    "    \n",
    "#ambil nilai shape spectogram\n",
    "images, input_shape = preprocess_spectogram(train_df.spectogram.values)\n",
    "\n",
    "# print (images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "#input shape\n",
    "shapes_spec = (images.shape[1],images.shape[2],4)\n",
    "input_shape_spec =layers.Input(shape= shapes_spec)\n",
    "\n",
    "print(input_shape_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##menentukan data dan label yang akan digunakan training\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "x_train_spec = images\n",
    "\n",
    "# labels - convert class vectors to binary class matrices One Hot Encoding\n",
    "labels = train_df.labels_num.values\n",
    "labels = to_categorical(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_spec, labels, test_size = 0.2, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Buat fungsi model CNN \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "def cnnmodels(input_shape_spec):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(shapes_spec)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(2))\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnnmodels(input_shape_spec)\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training model CNN\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "model.load_weights('best_model.h5')\n",
    "\n",
    "final_loss, final_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Final loss: {0:.6f}, final accuracy: {1:.6f}\".format(final_loss, final_acc))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss, final_acc = model.evaluate([x_test], y_test,batch_size=128, verbose=1)\n",
    "print(\"\\nTest data, accuracy: {:5.2f}%\".format(100*final_acc))\n",
    "print(\"\\nTest data, loss: {:5.2f}%\".format(100*final_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusiion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = model.predict([x_test]) \n",
    "y_pred = np.argmax(model.predict([x_test]),axis=1) \n",
    "predicted_test = np.argmax(y_test, axis=1)\n",
    "cm = confusion_matrix(predicted_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(predicted_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(history.history['accuracy'])\n",
    "ax.plot(history.history['val_accuracy'])\n",
    "ax.set_title('Model Accuracy')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(['train','test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Event','Noise'])\n",
    "disp.plot(cmap='Blues', values_format='')\n",
    "plt.title(f'Classification CNN Results ({epochs} epochs)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('M1(confusion_matrix).png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.read_csv('/Users/litbanggeo/Documents/Kerjaan_litbang/Kegiatan_2023/BAHAN_AJAR_PUSDIKLAT26AGUSTUS23/eq_cnn/labels.csv')\n",
    "predicted_df['spectogram'] = predicted_df.files_name.map(lambda id: f'dataset/spectogram_2/{id}.png')\n",
    "\n",
    "imagesP, input_shapeP = preprocess_spectogram(predicted_df.spectogram.values)\n",
    "\n",
    "classTarget = [\n",
    "    'Event', \n",
    "    'Noise', \n",
    "]\n",
    "\n",
    "y_pred = model.predict(imagesP) \n",
    "print(' prediksi data adalah ## {} ##'.format(classTarget[np.argmax(y_pred[103])]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
